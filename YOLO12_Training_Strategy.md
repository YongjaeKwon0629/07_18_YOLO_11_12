# ğŸ¯ YOLO V12: í•™ìŠµ ë° ìµœì í™” ì „ëµ (Training & Optimization Strategy) 

---

## 1. í•™ìŠµ ì…‹ì—… ê°œìš”

YOLO V12ëŠ” **Self-supervised Learning, ê³ ê¸‰ Data Augmentation, íš¨ê³¼ì ì¸ Loss ë° Regularization, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**ë¥¼ ë„ì…í•˜ì—¬ ì‹¤ì œ ë°ì´í„° í™˜ê²½ì— íŠ¹í™”ëœ GeneralizationÂ·Robustnessë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  
CNN íŠ¸ë Œë“œì™€ íŠ¸ëœìŠ¤í¬ë¨¸ ì´ë¡ ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ì— ìµœì í™”ëœ í•™ìŠµ ë° íŠœë‹ ë°©ë²•ë¡ ì´ ì ìš©ë©ë‹ˆë‹¤.

---

## 2. ë°ì´í„° ì „ì²˜ë¦¬ Â· ì¦ê°• ì „ëµ

### 2.1 Advanced Data Augmentation 

- **Mosaic, MixUp, CutMix**  
  - ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•©ì„±(Mosaic)í•˜ê±°ë‚˜, ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ í”½ì…€ ë‹¨ìœ„/íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ê²°í•©(MixUp/CutMix)
  - ë“œë¬¸/ì†Œìˆ˜ ê°ì²´ì™€ ë‹¤ì–‘í•œ ë°°ê²½ í˜¼í•© â†’ ëª¨ë¸ì˜ ê°•ì¸ì„± ë° ë°ì´í„° ë‹¤ì–‘ì„± ìƒìŠ¹
- **RandAugment, AutoAugment, GridMask**  
  - ìë™ ì •ì±… íƒìƒ‰ ê¸°ë°˜(AutoAugment) Â· ëœë¤ ì¦ê°• ì—°ì‚° ë°°ì¹˜(RandAugment)  
  - GridMaskë¡œ ì¼ë¶€ íŒ¨í„´ ì •ë³´ ì˜ë„ì ìœ¼ë¡œ ì†Œê±°í•˜ì—¬ ì˜¤ë²„í”¼íŒ… ë°©ì§€
- **Advanced Geometric/Photometric ë³€í˜•**  
  - Random Crop, Rotation, Shear, Affine ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ê¸°í•˜í•™ì  ë³€í˜•  
  - ë°ê¸°Â·ì±„ë„Â·ìƒ‰ìƒ ì „ì´(Color Jitter/HSV adjust) ë“± ê´‘í•™ íŠ¹ì„± ë³€í™˜

#### ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ
| ì „ëµ        | ì£¼ìš” íš¨ê³¼        | ì ìš© ë¹ˆë„ |
|-------------|----------------|----------|
| Mosaic      | ì†Œìˆ˜ ê°ì²´/ë³µí•© ë°°ê²½ í•™ìŠµ | 0.5 |
| MixUp       | í´ë˜ìŠ¤ê°„ í˜¼í•© Â· ê·¹ë‹¨ê°’ ëŒ€ì²˜ | 0.2 |
| GridMask    | êµ­ì†Œ ë°ì´í„° ì‚­ì œ Â· Regularization | 0.3 |
| Affine      | ë°©í–¥/í¬ê¸° ë¶ˆë³€ì„± | í•­ìƒ     |

```
transform = Compose([
Mosaic(prob=0.5),
MixUp(prob=0.2),
GridMask(prob=0.3),
RandomAffine(degrees=10, scale=(0.5,1.5)),
ColorJitter(hue=0.1, saturation=0.4, brightness=0.3),
RandomHorizontalFlip(p=0.5),
])
```

---

## 3. Self-Supervised ë° Pre-training ë„ì…

- **ìì²´ Jigsaw, Inpainting, Contrastive Learning**  
  - ë¼ë²¨ ì—†ëŠ” ì´ë¯¸ì§€ì— ëŒ€í•´ Jigsaw(ì¡°ê° ë§ì¶”ê¸°), Inpainting(ë§ˆìŠ¤í‚¹), SimCLRÂ·BYOL ë“± ëŒ€ì¡° í•™ìŠµ êµ¬í˜„  
  - ì‘ì€ ë¼ë²¨ ë°ì´í„°ì—ì„œë„ ê°•ê±´í•œ í”¼ì²˜ ì¶”ì¶œê¸°(Backbone) ì‚¬ì „í•™ìŠµ ê°€ëŠ¥  
- **Domain Adaptive Pre-training**  
  - ëŒ€ê·œëª¨ ê³µê°œ ë°ì´í„°(ì˜ˆ: ImageNet, OpenImages) ë° íƒ€ìŠ¤í¬ íŠ¹ì„±(í•­ê³µ, ì˜ë£Œ ë“±)ì— ë§ì¶˜ ë„ë©”ì¸ë³„ í”„ë¦¬íŠ¸ë ˆì¸

---

## 4. ìµœì  Loss Function ë° Training Objective

### 4.1 ì†ì‹¤ í•¨ìˆ˜ ê³µì‹ (Loss Functions)

| Loss          | ê³µì‹/êµ¬ì„±                        | ì ìš© ì´ìœ             |
|---------------|----------------------------------|--------------------|
| Focal Loss    | Class/Conf imbalance ì™„í™”         | ë“œë¬¸ í´ë˜ìŠ¤, Hard Example ì§‘ì¤‘ í•™ìŠµ |
| GIoU/DIoU/CIoU| Bounding Box ìœ„ì¹˜, í˜•íƒœ ì‹¬ì¸µ ì •í•© | Bbox ë¯¸ì„¸ ì¡°ì •, Robust Detection   |
| Distribution Focal Loss | Soft target ë¶„í¬ í™œìš©      | ì˜ˆì¸¡ ë¶„í¬ì˜ Detail ê°•í™”           |
| BCE Loss      | ì˜¤ë¸Œì íŠ¸ ì¡´ì¬/ë¯¸ì¡´ì¬ ì´ì§„ ë¶„ë¥˜     | Objectness ì„¤ê³„ì˜ í‘œì¤€           |

#### ë³µí•© Loss êµ¬ì„± ì˜ˆ:

```
total_loss = lambda1 * classification_loss
+ lambda2 * localization_loss
+ lambda3 * objectness_loss
+ regularization_penalty
```
*ëŒë‹¤ ê°€ì¤‘ì¹˜ëŠ” ì‹¤í—˜ì ìœ¼ë¡œ ë°ì´í„°ì…‹ ë° ëª©ì ì— ë”°ë¼ ì¡°ì •*

---

## 5. Regularization Â· ì¼ë°˜í™” ì „ëµ

- **Label Smoothing, Mixup Regularization**  
  - Noise label ìƒì„± í†µí•œ ê²½ê³„ ì™„í™”, Confidence ì˜ˆì¸¡ í¸í–¥ ë°©ì§€
- **DropBlock, Spatial Dropout**  
  - Feature map ì¼ë¶€ drop â†’ íŠ¹ì • í”¼ì²˜/ìœ„ì¹˜ ê³¼ì˜ì¡´ ë°©ì§€
- **EMA(Exponential Moving Average) Weight ì—…ë°ì´íŠ¸**  
  - ìŠ¤í…ë³„ íŒŒë¼ë¯¸í„° í‰ê· í™”ë¡œ í•™ìŠµ ê·¹í›„ë°˜ ì•ˆì •ì„±, ìµœê³  ì •í™•ë„ ë‹¬ì„±

---

## 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ìë™ ìµœì í™”

### 6.1 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìš”ì†Œ

- **Learning rate/Loss Weight/Optimizer**  
  - Cosine Annealing/LR Warmup/One Cycle ë“± ì„ í˜• ë° ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ê´€ë¦¬
  - AdamW, LAMB, SGD(Momentum) ë“± ìƒí™©ë³„ ë§ì¶¤ ìµœì í™”
- **ì´ì§„/ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë©€í‹° ë°°ì¹˜ ì‚¬ì´ì¦ˆ**  
  - Mixed Precision Training(Apex/AMP ë“±)ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ë° í•™ìŠµì†ë„ ê·¹ëŒ€í™”
- **AutoML/Hyperopt í™œìš©**  
  - Optuna, RayTune, KerasTuner ê¸°ë°˜ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
  - grid/random/bayesian search ì¡°í•©

### 6.2 ìµœì í™” ìŠ¤ì¼€ì¤„ ìƒ˜í”Œ

| ë‹¨ê³„          | ì£¼ìš” ì¡°ì •                        |
|---------------|-------------------------------|
| 1~10epoch     | LR Warmup(ì €/ì ì§„)             |
| ì¤‘ë°˜ê¸°        | Cosine Annealing/Cyclical LR   |
| í›„ê¸°          | EMA ì ìš©, Early Stopping ì ê·¹ í™œìš© |

---

## 7. ë¶„ì‚°Â·ëŒ€ê·œëª¨ í•™ìŠµ (Scalable Training)

- **DistributedDataParallel, Horovod, DeepSpeed**  
  - ë‹¤ìˆ˜ì˜ GPU/Nodeì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ê·œëª¨ í•™ìŠµ  
  - Gradient Accumulation Â· Mixed-precision ë™ì‹œ ì ìš©
- **SyncBN, Multi-node io**  
  - BatchNorm íŒŒë¼ë¯¸í„° ë™ê¸°í™”, ëŒ€ê·œëª¨ ë°ì´í„° IO íš¨ìœ¨í™”

#### íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ì˜ˆì‹œ

```
import torch.nn.parallel
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)
```

*ì‹¤ì „ í”„ë¡œì íŠ¸ì—ì„œëŠ” Slurm, kubeflow, AWS Sagemaker, GCP ë“± í”Œë«í¼ í™œìš©*

---

## 8. ì‹¤ì „ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

```
for images, targets in dataloader:
outputs = model(images)
loss_cls = focal_loss(outputs, targets)
loss_loc = ciou_loss(outputs, targets)
loss_obj = bce_loss(outputs, targets)
total_loss = (loss_cls + loss_loc + loss_obj + regularization_penalty)
optimizer.zero_grad()
total_loss.backward()
optimizer.step()
lr_scheduler.step()
if use_ema:
ema.update(model)
```

---

## 9. ì´ë¡ Â·ì‹¤ì „ì  ì‹œì‚¬ì  ìš”ì•½

- ì²¨ë‹¨ ë°ì´í„° ì¦ê°•, ì†ì‹¤ í•¨ìˆ˜ ì „ëµ, Self-supervised ì‚¬ì „í•™ìŠµ, ë‹¤ë‹¨ê³„ ì •êµ íŠœë‹ì˜ ìœµí•©ì´ YOLO V12ì˜ í•µì‹¬ ê²½ìŸë ¥
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë¶„ì‚°Â·í˜¼í•©ì •ë°€(MP) í•™ìŠµ ì ê·¹ ì ìš© ì‹œ ëŒ€ê·œëª¨ AIÂ·ì—£ì§€ í™˜ê²½ ë“± ì°¨ì„¸ëŒ€ ì‘ìš©ì— ì‹ ì† ëŒ€ì‘ ê°€ëŠ¥
- ê³„ì†ëœ ë…¼ë¬¸/ì˜¤í”ˆì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí‚¹Â·ìë™í™” ë„êµ¬ í™œìš©ì´ ë¯¸ë˜ ê°ì²´ íƒì§€ ì„±ëŠ¥ì„ ì¢Œìš°

---
